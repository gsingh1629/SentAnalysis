{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "%matplotlib inline\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "df = pd.read_csv(\"Data.csv\")\r\n",
    "df.head(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df['sentiment'] = np.where(df['sentiment'] == \"positive\", 1, 0)\r\n",
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df['sentiment'].value_counts().sort_index().plot(kind='bar',color = 'blue')\r\n",
    "plt.xlabel('Sentiment')\r\n",
    "plt.ylabel('Count')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Count')"
      ]
     },
     "metadata": {},
     "execution_count": 4
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "df = df.sample(frac=0.1, random_state=0) \r\n",
    "df.dropna(inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'],test_size=0.1, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def cleanText(raw_text, remove_stopwords=False, stemming=False, split_text=False):\r\n",
    "    text = BeautifulSoup(raw_text, 'html.parser').get_text()\r\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\r\n",
    "    words = letters_only.lower().split() \r\n",
    "    \r\n",
    "    if remove_stopwords:\r\n",
    "        stops = set(stopwords.words(\"english\"))\r\n",
    "        words = [w for w in words if not w in stops]\r\n",
    "        \r\n",
    "    if stemming==True:\r\n",
    "\r\n",
    "        stemmer = SnowballStemmer('english') \r\n",
    "        words = [stemmer.stem(w) for w in words]\r\n",
    "        \r\n",
    "    if split_text==True:\r\n",
    "        return (words)\r\n",
    "    \r\n",
    "    return( \" \".join(words))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "import re\r\n",
    "import nltk\r\n",
    "from nltk.corpus import stopwords \r\n",
    "from nltk.stem.porter import PorterStemmer\r\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\r\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\r\n",
    "from bs4 import BeautifulSoup \r\n",
    "import logging\r\n",
    "from wordcloud import WordCloud\r\n",
    "from gensim.models import word2vec\r\n",
    "from gensim.models import Word2Vec\r\n",
    "from gensim.models.keyedvectors import KeyedVectors\r\n",
    "\r\n",
    "X_train_cleaned = []\r\n",
    "X_test_cleaned = []\r\n",
    "\r\n",
    "for d in X_train:\r\n",
    "    X_train_cleaned.append(cleanText(d))\r\n",
    "print('Show a cleaned review in the training set : \\n',  X_train_cleaned[10])\r\n",
    "    \r\n",
    "for d in X_test:\r\n",
    "    X_test_cleaned.append(cleanText(d))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Show a cleaned review in the training set : \n",
      " the crimson rivers is one of the most over directed over the top over everything mess i ve ever seen come out of france there s nothing worse than a french production trying to out do films made in hollywood and cr is a perfect example of such a wannabe horror action buddy flick i almost stopped it halfway through because i knew it wouldn t amount to anything but french guys trying to show off the film starts off promisingly like some sort of expansive horror film but it quickly shifts genres from horror to action to x files type to buddy flick that in the end cr is all of it and also none of it it s so full of clich s that at one point i thought the whole thing was a comedy the painful dialogue and those silent pauses with fades outs and fades ins just at the right expositionary moments made me groan i thought only films made in hollywood used this hackneyed technique the chase scene with vincent cassel running after the killer is so over directed and over done that it s almost a thing of beauty the climax on top of the mountain with the stupid revelation about the killer s with cassel and reno playing buddies like nolte and murphy in hrs completely derailed what little credibility the film had by then it s difficult to believe that the director of the crimson rivers also directed gothika which though had its share of problems doesn t even come close to the awfulness of this overbaked confused film\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CountVectorizer with Mulinomial Naive Bayes (Benchmark Model)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\r\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\r\n",
    "countVect = CountVectorizer() \r\n",
    "X_train_countVect = countVect.fit_transform(X_train_cleaned)\r\n",
    "print(\"Number of features : %d \\n\" %len(countVect.get_feature_names())) #6378 \r\n",
    "print(\"Show some feature names : \\n\", countVect.get_feature_names()[::1000])\r\n",
    "\r\n",
    "\r\n",
    "# Train MultinomialNB classifier\r\n",
    "mnb = MultinomialNB()\r\n",
    "mnb.fit(X_train_countVect, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features : 36751 \n",
      "\n",
      "Show some feature names : \n",
      " ['aa', 'ameche', 'auggie', 'betrayals', 'bright', 'cathryn', 'clownhouse', 'copying', 'dazzle', 'disarray', 'dvd', 'estimation', 'fighter', 'fusion', 'greenfinch', 'henson', 'imaginings', 'ir', 'kint', 'linklater', 'maropis', 'misik', 'nectar', 'organise', 'performing', 'pre', 'rages', 'reputedly', 'saddled', 'sexiness', 'smith', 'steal', 'swoozie', 'tinfoil', 'unattuned', 'vernacular', 'willed']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import pickle\r\n",
    "pickle.dump(countVect,open('countVect_imdb.pkl','wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "from sklearn import metrics\r\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score\r\n",
    "def modelEvaluation(predictions):\r\n",
    "    '''\r\n",
    "    Print model evaluation to predicted result \r\n",
    "    '''\r\n",
    "    print (\"\\nAccuracy on validation set: {:.4f}\".format(accuracy_score(y_test, predictions)))\r\n",
    "    print(\"\\nAUC score : {:.4f}\".format(roc_auc_score(y_test, predictions)))\r\n",
    "    print(\"\\nClassification report : \\n\", metrics.classification_report(y_test, predictions))\r\n",
    "    print(\"\\nConfusion Matrix : \\n\", metrics.confusion_matrix(y_test, predictions))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "predictions = mnb.predict(countVect.transform(X_test_cleaned))\r\n",
    "modelEvaluation(predictions)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Accuracy on validation set: 0.8140\n",
      "\n",
      "AUC score : 0.8142\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.86      0.82       249\n",
      "           1       0.85      0.77      0.81       251\n",
      "\n",
      "    accuracy                           0.81       500\n",
      "   macro avg       0.82      0.81      0.81       500\n",
      "weighted avg       0.82      0.81      0.81       500\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[214  35]\n",
      " [ 58 193]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "import pickle\r\n",
    "pickle.dump(mnb,open('Naive_Bayes_model_imdb.pkl','wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TfidfVectorizer with Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from sklearn.linear_model import LogisticRegression\r\n",
    "tfidf = TfidfVectorizer(min_df=5) #minimum document frequency of 5\r\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\r\n",
    "print(\"Number of features : %d \\n\" %len(tfidf.get_feature_names())) #1722\r\n",
    "print(\"Show some feature names : \\n\", tfidf.get_feature_names()[::1000])\r\n",
    "\r\n",
    "# Logistic Regression\r\n",
    "lr = LogisticRegression()\r\n",
    "lr.fit(X_train_tfidf, y_train)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Number of features : 10505 \n",
      "\n",
      "Show some feature names : \n",
      " ['00', 'belonged', 'completion', 'dubious', 'garbage', 'interviewing', 'million', 'plays', 'rough', 'strike', 'vein']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "feature_names = np.array(tfidf.get_feature_names())\r\n",
    "sorted_coef_index = lr.coef_[0].argsort()\r\n",
    "print('\\nTop 10 features with smallest coefficients :\\n{}\\n'.format(feature_names[sorted_coef_index[:10]]))\r\n",
    "print('Top 10 features with largest coefficients : \\n{}'.format(feature_names[sorted_coef_index[:-11:-1]]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Top 10 features with smallest coefficients :\n",
      "['bad' 'worst' 'awful' 'no' 'waste' 'poor' 'terrible' 'boring' 'even'\n",
      " 'minutes']\n",
      "\n",
      "Top 10 features with largest coefficients : \n",
      "['great' 'and' 'excellent' 'best' 'it' 'wonderful' 'very' 'also' 'well'\n",
      " 'love']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "predictions = lr.predict(tfidf.transform(X_test_cleaned))\r\n",
    "modelEvaluation(predictions)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Accuracy on validation set: 0.8500\n",
      "\n",
      "AUC score : 0.8500\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.85      0.85       249\n",
      "           1       0.85      0.85      0.85       251\n",
      "\n",
      "    accuracy                           0.85       500\n",
      "   macro avg       0.85      0.85      0.85       500\n",
      "weighted avg       0.85      0.85      0.85       500\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[211  38]\n",
      " [ 37 214]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "from sklearn.model_selection import  GridSearchCV\r\n",
    "from sklearn import metrics\r\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\r\n",
    "from sklearn.pipeline import Pipeline\r\n",
    "estimators = [(\"tfidf\", TfidfVectorizer()), (\"lr\", LogisticRegression())]\r\n",
    "model = Pipeline(estimators)\r\n",
    "\r\n",
    "\r\n",
    "params = {\"lr__C\":[0.1, 1, 10], \r\n",
    "          \"tfidf__min_df\": [1, 3], \r\n",
    "          \"tfidf__max_features\": [1000, None], \r\n",
    "          \"tfidf__ngram_range\": [(1,1), (1,2)], \r\n",
    "          \"tfidf__stop_words\": [None, \"english\"]} \r\n",
    "\r\n",
    "grid = GridSearchCV(estimator=model, param_grid=params, scoring=\"accuracy\", n_jobs=-1)\r\n",
    "grid.fit(X_train_cleaned, y_train)\r\n",
    "print(\"The best paramenter set is : \\n\", grid.best_params_)\r\n",
    "\r\n",
    "\r\n",
    "# Evaluate on the validaton set\r\n",
    "predictions = grid.predict(X_test_cleaned)\r\n",
    "modelEvaluation(predictions)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The best paramenter set is : \n",
      " {'lr__C': 10, 'tfidf__max_features': None, 'tfidf__min_df': 3, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': None}\n",
      "\n",
      "Accuracy on validation set: 0.8720\n",
      "\n",
      "AUC score : 0.8720\n",
      "\n",
      "Classification report : \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87       249\n",
      "           1       0.87      0.88      0.87       251\n",
      "\n",
      "    accuracy                           0.87       500\n",
      "   macro avg       0.87      0.87      0.87       500\n",
      "weighted avg       0.87      0.87      0.87       500\n",
      "\n",
      "\n",
      "Confusion Matrix : \n",
      " [[216  33]\n",
      " [ 31 220]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word2Vec\n",
    "<br>\n",
    "\n",
    "**Step 1 : Parse review text to sentences (Word2Vec model takes a list of sentences as inputs)**\n",
    "\n",
    "**Step 2 : Create volcabulary list using Word2Vec model.**\n",
    "\n",
    "**Step 3 : Transform each review into numerical representation by computing average feature vectors of words therein.**\n",
    "\n",
    "**Step 4 : Fit the average feature vectors to Random Forest Classifier.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\r\n",
    "\r\n",
    "def parseSent(review, tokenizer, remove_stopwords=False):\r\n",
    "\r\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\r\n",
    "    sentences = []\r\n",
    "    for raw_sentence in raw_sentences:\r\n",
    "        if len(raw_sentence) > 0:\r\n",
    "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\r\n",
    "    return sentences\r\n",
    "\r\n",
    "\r\n",
    "# Parse each review in the training set into sentences\r\n",
    "sentences = []\r\n",
    "for review in X_train_cleaned:\r\n",
    "    sentences += parseSent(review, tokenizer,remove_stopwords=False)\r\n",
    "    \r\n",
    "print('%d parsed sentence in the training set\\n'  %len(sentences))\r\n",
    "print('Show a parsed sentence in the training set : \\n',  sentences[10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4500 parsed sentence in the training set\n",
      "\n",
      "Show a parsed sentence in the training set : \n",
      " ['the', 'crimson', 'rivers', 'is', 'one', 'of', 'the', 'most', 'over', 'directed', 'over', 'the', 'top', 'over', 'everything', 'mess', 'i', 've', 'ever', 'seen', 'come', 'out', 'of', 'france', 'there', 's', 'nothing', 'worse', 'than', 'a', 'french', 'production', 'trying', 'to', 'out', 'do', 'films', 'made', 'in', 'hollywood', 'and', 'cr', 'is', 'a', 'perfect', 'example', 'of', 'such', 'a', 'wannabe', 'horror', 'action', 'buddy', 'flick', 'i', 'almost', 'stopped', 'it', 'halfway', 'through', 'because', 'i', 'knew', 'it', 'wouldn', 't', 'amount', 'to', 'anything', 'but', 'french', 'guys', 'trying', 'to', 'show', 'off', 'the', 'film', 'starts', 'off', 'promisingly', 'like', 'some', 'sort', 'of', 'expansive', 'horror', 'film', 'but', 'it', 'quickly', 'shifts', 'genres', 'from', 'horror', 'to', 'action', 'to', 'x', 'files', 'type', 'to', 'buddy', 'flick', 'that', 'in', 'the', 'end', 'cr', 'is', 'all', 'of', 'it', 'and', 'also', 'none', 'of', 'it', 'it', 's', 'so', 'full', 'of', 'clich', 's', 'that', 'at', 'one', 'point', 'i', 'thought', 'the', 'whole', 'thing', 'was', 'a', 'comedy', 'the', 'painful', 'dialogue', 'and', 'those', 'silent', 'pauses', 'with', 'fades', 'outs', 'and', 'fades', 'ins', 'just', 'at', 'the', 'right', 'expositionary', 'moments', 'made', 'me', 'groan', 'i', 'thought', 'only', 'films', 'made', 'in', 'hollywood', 'used', 'this', 'hackneyed', 'technique', 'the', 'chase', 'scene', 'with', 'vincent', 'cassel', 'running', 'after', 'the', 'killer', 'is', 'so', 'over', 'directed', 'and', 'over', 'done', 'that', 'it', 's', 'almost', 'a', 'thing', 'of', 'beauty', 'the', 'climax', 'on', 'top', 'of', 'the', 'mountain', 'with', 'the', 'stupid', 'revelation', 'about', 'the', 'killer', 's', 'with', 'cassel', 'and', 'reno', 'playing', 'buddies', 'like', 'nolte', 'and', 'murphy', 'in', 'hrs', 'completely', 'derailed', 'what', 'little', 'credibility', 'the', 'film', 'had', 'by', 'then', 'it', 's', 'difficult', 'to', 'believe', 'that', 'the', 'director', 'of', 'the', 'crimson', 'rivers', 'also', 'directed', 'gothika', 'which', 'though', 'had', 'its', 'share', 'of', 'problems', 'doesn', 't', 'even', 'come', 'close', 'to', 'the', 'awfulness', 'of', 'this', 'overbaked', 'confused', 'film']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating Volcabulary List usinhg Word2Vec Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from wordcloud import WordCloud\r\n",
    "from gensim.models import word2vec\r\n",
    "from gensim.models.keyedvectors import KeyedVectors\r\n",
    "num_features = 300  #embedding dimension                     \r\n",
    "min_word_count = 10                \r\n",
    "num_workers = 4       \r\n",
    "context = 10                                                                                          \r\n",
    "downsampling = 1e-3 \r\n",
    "\r\n",
    "print(\"Training Word2Vec model ...\\n\")\r\n",
    "w2v = Word2Vec(sentences, workers=num_workers, min_count = min_word_count,\\\r\n",
    "                 window = context, sample = downsampling)\r\n",
    "w2v.init_sims(replace=True)\r\n",
    "w2v.save(\"w2v_300features_10minwordcounts_10context\") #save trained word2vec model\r\n",
    "\r\n",
    "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index2word)) #4016 \r\n",
    "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index2word[0:10])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training Word2Vec model ...\n",
      "\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "The index2word attribute has been replaced by index_to_key since Gensim 4.0.0.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-a1423dc3c86e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"w2v_300features_10minwordcounts_10context\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#save trained word2vec model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Number of words in the vocabulary list : %d \\n\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#4016\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mindex2word\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mindex2word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m         raise AttributeError(\n\u001b[0m\u001b[0;32m    651\u001b[0m             \u001b[1;34m\"The index2word attribute has been replaced by index_to_key since Gensim 4.0.0.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[1;34m\"See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: The index2word attribute has been replaced by index_to_key since Gensim 4.0.0.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Averaging Feature Vectors"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def makeFeatureVec(review, model, num_features):\r\n",
    "    '''\r\n",
    "    Transform a review to a feature vector by averaging feature vectors of words \r\n",
    "    appeared in that review and in the volcabulary list created\r\n",
    "    '''\r\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\r\n",
    "    nwords = 0.\r\n",
    "    index2word_set = set(model.wv.index2word) #index2word is the volcabulary list of the Word2Vec model\r\n",
    "    isZeroVec = True\r\n",
    "    for word in review:\r\n",
    "        if word in index2word_set: \r\n",
    "            nwords = nwords + 1.\r\n",
    "            featureVec = np.add(featureVec, model[word])\r\n",
    "            isZeroVec = False\r\n",
    "    if isZeroVec == False:\r\n",
    "        featureVec = np.divide(featureVec, nwords)\r\n",
    "    return featureVec"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\r\n",
    "    '''\r\n",
    "    Transform all reviews to feature vectors using makeFeatureVec()\r\n",
    "    '''\r\n",
    "    counter = 0\r\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\r\n",
    "    for review in reviews:\r\n",
    "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\r\n",
    "        counter = counter + 1\r\n",
    "    return reviewFeatureVecs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "X_train_cleaned = []\r\n",
    "for review in X_train:\r\n",
    "    X_train_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\r\n",
    "trainVector = getAvgFeatureVecs(X_train_cleaned, w2v, num_features)\r\n",
    "print(\"Training set : %d feature vectors with %d dimensions\" %trainVector.shape)\r\n",
    "\r\n",
    "\r\n",
    "# Get feature vectors for validation set\r\n",
    "X_test_cleaned = []\r\n",
    "for review in X_test:\r\n",
    "    X_test_cleaned.append(cleanText(review, remove_stopwords=True, split_text=True))\r\n",
    "testVector = getAvgFeatureVecs(X_test_cleaned, w2v, num_features)\r\n",
    "print(\"Validation set : %d feature vectors with %d dimensions\" %testVector.shape)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "The index2word attribute has been replaced by index_to_key since Gensim 4.0.0.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-8ac92a3f1987>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mX_train_cleaned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleanText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mremove_stopwords\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_text\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrainVector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_cleaned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Training set : %d feature vectors with %d dimensions\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mtrainVector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-324b7777be5b>\u001b[0m in \u001b[0;36mgetAvgFeatureVecs\u001b[1;34m(reviews, model, num_features)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mreviewFeatureVecs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreviews\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mreviewFeatureVecs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcounter\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmakeFeatureVec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcounter\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mreviewFeatureVecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-726e33855246>\u001b[0m in \u001b[0;36mmakeFeatureVec\u001b[1;34m(review, model, num_features)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mfeatureVec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mnwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mindex2word_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#index2word is the volcabulary list of the Word2Vec model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0misZeroVec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreview\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mindex2word\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    648\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mindex2word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m         raise AttributeError(\n\u001b[0m\u001b[0;32m    651\u001b[0m             \u001b[1;34m\"The index2word attribute has been replaced by index_to_key since Gensim 4.0.0.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m             \u001b[1;34m\"See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: The index2word attribute has been replaced by index_to_key since Gensim 4.0.0.\nSee https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest Classifer"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "rf = RandomForestClassifier(n_estimators=1000)\r\n",
    "rf.fit(trainVector, y_train)\r\n",
    "predictions = rf.predict(testVector)\r\n",
    "modelEvaluation(predictions)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'trainVector' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-f174b646ac9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainVector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestVector\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodelEvaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainVector' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM\n",
    "<br>\n",
    "\n",
    "**Step 1 : Prepare X_train and X_test to 2D tensor.**\n",
    "    \n",
    "**Step 2 : Train a simple LSTM (embeddign layer => LSTM layer => dense layer).**\n",
    "    \n",
    "**Step 3 : Compile and fit the model using log loss function and ADAM optimizer.**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from keras.preprocessing import sequence\r\n",
    "from keras.utils import np_utils\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers.core import Dense, Dropout, Activation, Lambda\r\n",
    "from keras.layers.embeddings import Embedding\r\n",
    "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\r\n",
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from collections import defaultdict\r\n",
    "from keras.layers.convolutional import Convolution1D\r\n",
    "from keras import backend as K\r\n",
    "from keras.layers.embeddings import Embedding"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.compat.v2.summary API due to missing TensorBoard installation.\n",
      "WARNING:root:Limited tf.summary API due to missing TensorBoard installation.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top_words = 40000 \r\n",
    "maxlen = 200 \r\n",
    "batch_size = 62\r\n",
    "nb_classes = 4\r\n",
    "nb_epoch = 6\r\n",
    "\r\n",
    "\r\n",
    "# Vectorize X_train and X_test to 2D tensor\r\n",
    "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\r\n",
    "tokenizer.fit_on_texts(X_train)\r\n",
    "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\r\n",
    "\r\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\r\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\r\n",
    "\r\n",
    "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\r\n",
    "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\r\n",
    "\r\n",
    "\r\n",
    "# one-hot encoding of y_train and y_test\r\n",
    "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\r\n",
    "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\r\n",
    "\r\n",
    "print('X_train shape:', X_train_seq.shape)\r\n",
    "print(\"========================================\")\r\n",
    "print('X_test shape:', X_test_seq.shape)\r\n",
    "print(\"========================================\")\r\n",
    "print('y_train shape:', y_train_seq.shape)\r\n",
    "print(\"========================================\")\r\n",
    "print('y_test shape:', y_test_seq.shape)\r\n",
    "print(\"========================================\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train shape: (4500, 200)\n",
      "========================================\n",
      "X_test shape: (500, 200)\n",
      "========================================\n",
      "y_train shape: (4500, 4)\n",
      "========================================\n",
      "y_test shape: (500, 4)\n",
      "========================================\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model1 = Sequential()\r\n",
    "model1.add(Embedding(top_words, 128, dropout=0.2))\r\n",
    "model1.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \r\n",
    "model1.add(Dense(nb_classes))\r\n",
    "model1.add(Activation('softmax'))\r\n",
    "model1.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 128)         5120000   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 516       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 5,252,100\n",
      "Trainable params: 5,252,100\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model1.compile(loss='binary_crossentropy',\r\n",
    "              optimizer='adam',\r\n",
    "              metrics=['accuracy'])\r\n",
    "\r\n",
    "model1.fit(X_train_seq, y_train_seq, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\r\n",
    "\r\n",
    "# Model evluation\r\n",
    "score = model1.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\r\n",
    "print('Test loss : {:.4f}'.format(score[0]))\r\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/6\n",
      "4500/4500 [==============================] - 22s 5ms/step - loss: 0.3760 - accuracy: 0.7594\n",
      "Epoch 2/6\n",
      "4500/4500 [==============================] - 24s 5ms/step - loss: 0.2857 - accuracy: 0.8577\n",
      "Epoch 3/6\n",
      "4500/4500 [==============================] - 24s 5ms/step - loss: 0.1591 - accuracy: 0.9347\n",
      "Epoch 4/6\n",
      "4500/4500 [==============================] - 24s 5ms/step - loss: 0.0838 - accuracy: 0.9699\n",
      "Epoch 5/6\n",
      "4500/4500 [==============================] - 24s 5ms/step - loss: 0.0385 - accuracy: 0.9874\n",
      "Epoch 6/6\n",
      "4500/4500 [==============================] - 24s 5ms/step - loss: 0.0225 - accuracy: 0.9925\n",
      "500/500 [==============================] - 1s 1ms/step\n",
      "Test loss : 0.4559\n",
      "Test accuracy : 0.8750\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(X_train_seq),len(y_train_seq)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4500, 4500)"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Size of weight matrix in the embedding layer : \", \\\r\n",
    "      model1.layers[0].get_weights()[0].shape)\r\n",
    "\r\n",
    "# get weight matrix of the hidden layer\r\n",
    "print(\"Size of weight matrix in the hidden layer : \", \\\r\n",
    "      model1.layers[1].get_weights()[0].shape)\r\n",
    "\r\n",
    "# get weight matrix of the output layer\r\n",
    "print(\"Size of weight matrix in the output layer : \", \\\r\n",
    "      model1.layers[2].get_weights()[0].shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of weight matrix in the embedding layer :  (40000, 128)\n",
      "Size of weight matrix in the hidden layer :  (128, 512)\n",
      "Size of weight matrix in the output layer :  (128, 4)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\r\n",
    "pickle.dump(model1,open('model1.pkl','wb'))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LSTM with Word2Vec Embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "2v = Word2Vec.load(\"w2v_300features_10minwordcounts_10context\")\r\n",
    "\r\n",
    "embedding_matrix = w2v.wv.syn0 \r\n",
    "print(\"Shape of embedding matrix : \", embedding_matrix.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "top_words = embedding_matrix.shape[0] #4016 \r\n",
    "maxlen = 300 \r\n",
    "batch_size = 62\r\n",
    "nb_classes = 4\r\n",
    "nb_epoch = 7\r\n",
    "\r\n",
    "\r\n",
    "# Vectorize X_train and X_test to 2D tensor\r\n",
    "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\r\n",
    "tokenizer.fit_on_texts(X_train)\r\n",
    "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\r\n",
    "\r\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\r\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\r\n",
    "\r\n",
    "X_train_seq1 = sequence.pad_sequences(sequences_train, maxlen=maxlen)\r\n",
    "X_test_seq1 = sequence.pad_sequences(sequences_test, maxlen=maxlen)\r\n",
    "\r\n",
    "\r\n",
    "# one-hot encoding of y_train and y_test\r\n",
    "y_train_seq1 = np_utils.to_categorical(y_train, nb_classes)\r\n",
    "y_test_seq1 = np_utils.to_categorical(y_test, nb_classes)\r\n",
    "\r\n",
    "print('X_train shape:', X_train_seq1.shape)\r\n",
    "print(\"========================================\")\r\n",
    "print('X_test shape:', X_test_seq1.shape)\r\n",
    "print(\"========================================\")\r\n",
    "print('y_train shape:', y_train_seq1.shape)\r\n",
    "print(\"========================================\")\r\n",
    "print('y_test shape:', y_test_seq1.shape)\r\n",
    "print(\"========================================\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(X_train_seq1),len(y_train_seq1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedding_layer = Embedding(embedding_matrix.shape[0], #4016\r\n",
    "                            embedding_matrix.shape[1], #300\r\n",
    "                            weights=[embedding_matrix])\r\n",
    "\r\n",
    "model2 = Sequential()\r\n",
    "model2.add(embedding_layer)\r\n",
    "model2.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \r\n",
    "model2.add(Dense(nb_classes))\r\n",
    "model2.add(Activation('softmax'))\r\n",
    "model2.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model2.compile(loss='binary_crossentropy',\r\n",
    "              optimizer='adam',\r\n",
    "              metrics=['accuracy'])\r\n",
    "\r\n",
    "model2.fit(X_train_seq1, y_train_seq1, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\r\n",
    "\r\n",
    "# Model evaluation\r\n",
    "score = model2.evaluate(X_test_seq1, y_test_seq1, batch_size=batch_size)\r\n",
    "print('Test loss : {:.4f}'.format(score[0]))\r\n",
    "print('Test accuracy : {:.4f}'.format(score[1]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Size of weight matrix in the embedding layer : \", \\\r\n",
    "      model2.layers[0].get_weights()[0].shape) \r\n",
    "\r\n",
    "print(\"Size of weight matrix in the hidden layer : \", \\\r\n",
    "      model2.layers[1].get_weights()[0].shape) \r\n",
    "\r\n",
    "print(\"Size of weight matrix in the output layer : \", \\\r\n",
    "      model2.layers[2].get_weights()[0].shape) "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "e2511879089deb7775c3786d1b101cd439e3892c2ac9d771061833764ba1fe02"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}